{"dependencies":[{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\.babelrc","includedInParent":true,"mtime":1528197961732},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\package.json","includedInParent":true,"mtime":1528200878026},{"name":"@tensorflow/tfjs","loc":{"line":1,"column":20}},{"name":"./data","loc":{"line":2,"column":24}},{"name":"./ui","loc":{"line":3,"column":20}}],"generated":{"js":"'use strict';\n\nvar _tfjs = require('@tensorflow/tfjs');\n\nvar tf = _interopRequireWildcard(_tfjs);\n\nvar _data = require('./data');\n\nvar _ui = require('./ui');\n\nvar ui = _interopRequireWildcard(_ui);\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) newObj[key] = obj[key]; } } newObj.default = obj; return newObj; } }\n\nclass sampleLayer extends tf.layers.Layer {\n  constructor(args) {\n    super({});\n  }\n\n  computeOutputShape(inputShape) {\n    return inputShape[0];\n  }\n\n  call(inputs, kwargs) {\n    return tf.tidy(() => {\n      const [z_mean, z_log_var] = inputs;\n      const batch = z_mean.shape[0];\n      const dim = z_mean.shape[1];\n      const epsilon = tf.randomNormal([batch, dim]);\n      const half = tf.scalar(0.5);\n      const temp = z_log_var.mul(half).exp().mul(epsilon);\n      const sample = z_mean.add(temp);\n      return sample;\n    });\n  }\n\n  getClassName() {\n    return 'sampleLayer';\n  }\n}\n\nasync function train(config) {\n  // # network parameters\n  const input_shape = config.input_shape;\n  const original_dim = config.original_dim;\n  const intermediate_dim = config.intermediate_dim;\n  const batch_size = config.batch_size;\n  const num_batch = config.num_batch;\n  const latent_dim = config.latent_dim;\n  const epochs = config.epochs;\n  const test_batch_size = 1000;\n\n  // // # VAE model = encoder + decoder\n  // // # build encoder model\n  // const encoder_inputs = tf.input({shape: [original_dim]});\n  // const x_encoder = tf.layers.dense({units: intermediate_dim, activation: 'relu'}).apply(encoder_inputs);\n  // const z_mean = tf.layers.dense({units: latent_dim, name: 'z_mean'}).apply(x_encoder);\n  // const z_log_var = tf.layers.dense({units: latent_dim, name: 'z_log_var'}).apply(x_encoder);\n  //\n  // // # use reparameterization trick to push the sampling out as input\n  // // # note that \"output_shape\" isn't necessary with the TensorFlow backend\n  // const z = new sampleLayer().apply([z_mean, z_log_var]);\n  // const encoder = tf.model({\n  //   inputs: encoder_inputs,\n  //   outputs: [\n  //     z_mean, z_log_var, z\n  //   ],\n  //   name: \"encoder\"\n  // })\n  //\n  // // # build decoder model\n  // const decoder_inputs = tf.input({shape: [latent_dim]});\n  // const x_decoder = tf.layers.dense({units: intermediate_dim, activation: 'relu'}).apply(decoder_inputs);\n  // const decoder_outputs = tf.layers.dense({units: original_dim, activation: 'sigmoid'}).apply(x_decoder);\n  // const decoder = tf.model({inputs: decoder_inputs, outputs: decoder_outputs, name: \"decoder\"})\n\n  // # VAE model = encoder + decoder\n  // # build encoder model\n  const encoder_inputs = tf.input({ shape: [original_dim] });\n  const x1_l = tf.layers.dense({ units: intermediate_dim, kernelInitializer: 'glorotUniform' }).apply(encoder_inputs);\n  const x1_n = tf.layers.batchNormalization({ axis: 1 }).apply(x1_l);\n  const x1 = tf.layers.elu().apply(x1_n);\n  const z_mean = tf.layers.dense({ units: latent_dim, kernelInitializer: 'glorotUniform' }).apply(x1);\n  const z_log_var = tf.layers.dense({ units: latent_dim, kernelInitializer: 'glorotUniform' }).apply(x1);\n\n  // # use reparameterization trick to push the sampling out as input\n  // # note that \"output_shape\" isn't necessary with the TensorFlow backend\n  const z = new sampleLayer().apply([z_mean, z_log_var]);\n  const encoder = tf.model({\n    inputs: encoder_inputs,\n    outputs: [z_mean, z_log_var, z],\n    name: \"encoder\"\n  });\n\n  // # build decoder model\n  const decoder_inputs = tf.input({ shape: [latent_dim] });\n  const x2_l = tf.layers.dense({ units: intermediate_dim, kernelInitializer: 'glorotUniform' }).apply(decoder_inputs);\n  const x2_n = tf.layers.batchNormalization({ axis: 1 }).apply(x2_l);\n  const x2 = tf.layers.elu().apply(x2_n);\n  const decoder_outputs = tf.layers.dense({ units: original_dim, activation: 'sigmoid' }).apply(x2);\n  const decoder = tf.model({ inputs: decoder_inputs, outputs: decoder_outputs, name: \"decoder\" });\n\n  const vae = inputs => {\n    return tf.tidy(() => {\n      const [z_mean, z_log_var, z] = encoder.apply(inputs);\n      const outputs = decoder.apply(z);\n      return [z_mean, z_log_var, outputs];\n    });\n  };\n\n  const optimizer = tf.train.adam();\n\n  const reconstructionLoss = (yTrue, yPred) => {\n    return tf.tidy(() => {\n      let reconstruction_loss;\n      reconstruction_loss = tf.metrics.binaryCrossentropy(yTrue, yPred);\n      reconstruction_loss = reconstruction_loss.mul(tf.scalar(yPred.shape[1]));\n      return reconstruction_loss;\n    });\n  };\n\n  const klLoss = (z_mean, z_log_var) => {\n    return tf.tidy(() => {\n      let kl_loss;\n      kl_loss = tf.scalar(1).add(z_log_var).sub(z_mean.square()).sub(z_log_var.exp());\n      kl_loss = tf.sum(kl_loss, -1);\n      kl_loss = kl_loss.mul(tf.scalar(-0.5));\n      return kl_loss;\n    });\n  };\n\n  const vaeLoss = (yTrue, yPred) => {\n    return tf.tidy(() => {\n      // K.max(y_pred,0)-y_pred * y_true + K.log(1+K.exp((-1)*K.abs(y_pred)))\n      const [z_mean, z_log_var, y] = yPred;\n      // const reconstruction_loss = binaryCrossentropy(yTrue, y);\n      const reconstruction_loss = reconstructionLoss(yTrue, y);\n      const kl_loss = klLoss(z_mean, z_log_var);\n      const total_loss = tf.mean(reconstruction_loss.add(kl_loss));\n      return total_loss;\n    });\n  };\n\n  let batch_index = 0;\n  for (let i = 0; i < epochs; i++) {\n    let batcInput;\n    let testBatchInput;\n    let trainLoss;\n    let validationLoss;\n    let testBatchResult;\n    let epochLoss;\n\n    ui.logMessage(`[Epoch ${i + 1}]\\n`);\n    epochLoss = 0;\n    for (let j = 0; j < num_batch; j++) {\n      batchInput = data.nextTrainBatch(batch_size).xs.reshape([batch_size, original_dim]);\n      trainLoss = await optimizer.minimize(() => vaeLoss(batchInput, vae(batchInput)), true).data();\n      trainLoss = Number(trainLoss);\n      epochLoss = epochLoss + trainLoss;\n      ui.logMessage(`\\t[Batch ${j + 1}] Training Loss: ${trainLoss}.\\n`);\n      // ui.plotTrainLoss(trainLoss);\n      await tf.nextFrame();\n    }\n    epochLoss = epochLoss / num_batch;\n    ui.logMessage(`\\t[Average] Training Loss: ${epochLoss}.\\n`);\n    ui.updateProgressBar(i, epochs);\n    testBatchInput = data.nextTrainBatch(test_batch_size).xs.reshape([test_batch_size, original_dim]);\n    testBatchResult = vae(testBatchInput);\n    valLoss = await vaeLoss(testBatchInput, testBatchResult).data();\n    valLoss = Number(valLoss);\n    // ui.plotValLoss(valLoss);\n    await tf.nextFrame();\n  }\n\n  return [encoder, decoder];\n}\n\nlet isTrained = false;\nlet data;\nasync function load() {\n  data = new _data.MnistData();\n  await data.load();\n}\n\nlet encoder, decoder;\nasync function loadAndTrain() {\n  ui.setStatus('Loading data...\\n');\n  await load();\n\n  ui.setStatus('Training...\\n');\n  [encoder, decoder] = await train({\n    input_shape: 784,\n    original_dim: 784,\n    intermediate_dim: 512,\n    batch_size: 128,\n    num_batch: 50,\n    latent_dim: 2,\n    epochs: ui.getEpochs()\n  });\n  ui.setStatus('Model Trained.\\n');\n  isTrained = true;\n}\n\nasync function test() {\n  if (isTrained) {\n    const testSampleSize = ui.getTestSampleSize();\n    const zs = tf.randomNormal([testSampleSize, 2]);\n    const outputs = decoder.apply(zs);\n    await ui.showTestResults(zs, outputs);\n  }\n}\n\nfunction main() {\n  ui.setRetrainFunction(loadAndTrain);\n  ui.setTestFunction(test);\n}\n\nmain();"},"hash":"adac3f164fb19300c4ed61d9de47f198","cacheData":{"env":{}}}