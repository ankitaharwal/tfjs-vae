{"dependencies":[{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\package.json","includedInParent":true,"mtime":1528200878026},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\.babelrc","includedInParent":true,"mtime":1528197961732},{"name":"C:\\Users\\qison\\Google Drive\\tfjs_vae\\nodejs_version\\node_modules\\@tensorflow\\tfjs-layers\\package.json","includedInParent":true,"mtime":1524501157000},{"name":"@tensorflow/tfjs-core","loc":{"line":13,"column":26}},{"name":"../activations","loc":{"line":14,"column":28}},{"name":"../backend/tfjs_backend","loc":{"line":16,"column":29}},{"name":"../engine/topology","loc":{"line":17,"column":25}},{"name":"../errors","loc":{"line":18,"column":23}},{"name":"../types","loc":{"line":19,"column":22}},{"name":"../utils/generic_utils","loc":{"line":20,"column":28}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar activations_1 = require(\"../activations\");\nvar tfjs_backend_1 = require(\"../backend/tfjs_backend\");\nvar tfjs_backend_2 = require(\"../backend/tfjs_backend\");\nvar topology_1 = require(\"../engine/topology\");\nvar errors_1 = require(\"../errors\");\nvar types_1 = require(\"../types\");\nvar generic_utils = require(\"../utils/generic_utils\");\nvar LeakyReLU = (function (_super) {\n    __extends(LeakyReLU, _super);\n    function LeakyReLU(config) {\n        var _this = _super.call(this, config == null ? {} : config) || this;\n        _this.DEFAULT_ALPHA = 0.3;\n        if (config == null) {\n            config = {};\n        }\n        _this.alpha = config.alpha == null ? _this.DEFAULT_ALPHA : config.alpha;\n        return _this;\n    }\n    LeakyReLU.prototype.call = function (inputs, kwargs) {\n        var x = generic_utils.getExactlyOneTensor(inputs);\n        return tfjs_core_1.leakyRelu(x, this.alpha);\n    };\n    LeakyReLU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    LeakyReLU.prototype.getClassName = function () {\n        return 'LeakyReLU';\n    };\n    LeakyReLU.prototype.getConfig = function () {\n        var config = { alpha: this.alpha };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return LeakyReLU;\n}(topology_1.Layer));\nexports.LeakyReLU = LeakyReLU;\ngeneric_utils.ClassNameMap.register('LeakyReLU', LeakyReLU);\nvar ELU = (function (_super) {\n    __extends(ELU, _super);\n    function ELU(config) {\n        var _this = _super.call(this, config == null ? {} : config) || this;\n        _this.DEFAULT_ALPHA = 1.0;\n        if (config == null) {\n            config = {};\n        }\n        if (config.alpha != null && config.alpha !== _this.DEFAULT_ALPHA) {\n            throw new errors_1.NotImplementedError(\"Non-default alpha value (\" + config.alpha + \") is not supported by the \" +\n                \"ELU layer yet.\");\n        }\n        _this.alpha = config.alpha == null ? _this.DEFAULT_ALPHA : config.alpha;\n        return _this;\n    }\n    ELU.prototype.call = function (inputs, kwargs) {\n        var x = generic_utils.getExactlyOneTensor(inputs);\n        return tfjs_core_1.elu(x);\n    };\n    ELU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    ELU.prototype.getClassName = function () {\n        return 'ELU';\n    };\n    ELU.prototype.getConfig = function () {\n        var config = { alpha: this.alpha };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return ELU;\n}(topology_1.Layer));\nexports.ELU = ELU;\ngeneric_utils.ClassNameMap.register('ELU', ELU);\nvar ThresholdedReLU = (function (_super) {\n    __extends(ThresholdedReLU, _super);\n    function ThresholdedReLU(config) {\n        var _this = _super.call(this, config == null ? {} : config) || this;\n        _this.DEFAULT_THETA = 1.0;\n        if (config == null) {\n            config = {};\n        }\n        _this.theta = config.theta == null ? _this.DEFAULT_THETA : config.theta;\n        _this.thetaTensor = tfjs_backend_2.getScalar(_this.theta);\n        return _this;\n    }\n    ThresholdedReLU.prototype.call = function (inputs, kwargs) {\n        var x = generic_utils.getExactlyOneTensor(inputs);\n        return x.mul(tfjs_backend_1.cast(x.greater(this.thetaTensor), types_1.DType.float32));\n    };\n    ThresholdedReLU.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    ThresholdedReLU.prototype.getClassName = function () {\n        return 'ThresholdedReLU';\n    };\n    ThresholdedReLU.prototype.getConfig = function () {\n        var config = { theta: this.theta };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return ThresholdedReLU;\n}(topology_1.Layer));\nexports.ThresholdedReLU = ThresholdedReLU;\ngeneric_utils.ClassNameMap.register('ThresholdedReLU', ThresholdedReLU);\nvar Softmax = (function (_super) {\n    __extends(Softmax, _super);\n    function Softmax(config) {\n        var _this = _super.call(this, config == null ? {} : config) || this;\n        _this.DEFAULT_AXIS = 1.0;\n        if (config == null) {\n            config = {};\n        }\n        _this.axis = config.theta == null ? _this.DEFAULT_AXIS : config.theta;\n        return _this;\n    }\n    Softmax.prototype.call = function (inputs, kwargs) {\n        var x = generic_utils.getExactlyOneTensor(inputs);\n        return activations_1.softmax(x, this.axis);\n    };\n    Softmax.prototype.computeOutputShape = function (inputShape) {\n        return inputShape;\n    };\n    Softmax.prototype.getClassName = function () {\n        return 'Softmax';\n    };\n    Softmax.prototype.getConfig = function () {\n        var config = { axis: this.axis };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return Softmax;\n}(topology_1.Layer));\nexports.Softmax = Softmax;\ngeneric_utils.ClassNameMap.register('Softmax', Softmax);\n"},"hash":"9dded41c9b9324b0f2337b06cfeb46fe","cacheData":{"env":{}}}